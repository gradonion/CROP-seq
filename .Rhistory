#    theta: length 2 vector that stores: log(mu_j), log(inv_disp_j)
#    lambda: length K vector of fold changes for each class
#    X: length N vector that stores the expression of gene j in each sample
#    pi: length K vector of mixture probabilities for each class
#    qgamma: (N x K) posterior inclusion probabilities of gamma
mu = exp(theta[1])
inv_disp = exp(theta[2])
N = length(x)
K = length(pi)
llh = rep(NA,N)
for (i in 1:N){
llh[i] = sum(qgamma[i,]*(log(dnbinom(x[i], mu=mu*lambda, size=1/inv_disp))))
}
return(-1*sum(llh))
}
simTwoClassNB <- function(N, P, mu, inv_disp, pi, lambda, seed){
#  Returns NxP count matrix sampled from a mixture of NB distributions
#  Args:
#     N: cells (rows)
#     P: genes (columns)
#     mu : vector of gene means (length P)
#     inv_disp : vector of inverse dispersion parameters (length P)
#     pi : mixture proportions (assuming two classes)
#     lambda: copy number change
set.seed(seed)
count.mat <- matrix(0, nrow=N, ncol=P)
gamma = rbinom(N, 1, pi[1])
for (i in 1:N){
count.mat[i,] <- gamma[i] * rnbinom(n = P, mu = lambda[1]*mu, size = 1/inv_disp) +
(1-gamma[i]) * rnbinom(n = P, mu = lambda[2]*mu, size = 1/inv_disp)
}
return(list(X = count.mat, gamma1 = gamma))
}
#####
## Simulate data
N = 1000
P = 4
K = 2
mu.true = c(5,5,10,10)
inv_disp.true = rep(0.1,P)
pi = c(0.7,0.3)
lambda = c(1,1.5)
seed = 1
data = simTwoClassNB(N, P, mu.true, inv_disp.true, pi, lambda, seed)
X = data$X
gamma.true = data$gamma1
qgamma.true = Qgamma(X, mu.true, inv_disp.true, pi, lambda)
llh.true = rep(NA,P)
for (j in 1:P){
llh.true[j] = gene.LLH(theta=log(c(mu.true[j],inv_disp.true[j])), lambda, X[,j], qgamma.true)
}
LLH.true = sum(llh.true) - sum(log(pi)*t(qgamma.true))
set.seed(seed)
niter = 100
mu.update = matrix(NA,nrow = niter,ncol = P)
inv_disp.update = matrix(NA,nrow = niter,ncol = P)
qgamma1.update = matrix(NA,nrow = niter,ncol = N)
llh.update = rep(NA,niter)
mu.init = runif(P,min = 0,max = 50)
mu = mu.init
inv_disp.init = runif(P,min = 0,max = 10)
inv_disp = inv_disp.init
iter = 1
set.seed(seed)
niter = 20
mu.update = matrix(NA,nrow = niter,ncol = P)
inv_disp.update = matrix(NA,nrow = niter,ncol = P)
qgamma1.update = matrix(NA,nrow = niter,ncol = N)
llh.update = rep(NA,niter)
mu.init = runif(P,min = 0,max = 50)
mu = mu.init
inv_disp.init = runif(P,min = 0,max = 10)
inv_disp = inv_disp.init
iter = 1
while (iter <= niter) {
qgamma = Qgamma(X, mu, inv_disp, pi, lambda)
qgamma1.update[iter,] = qgamma[,1]
llh = rep(NA,P)
for (j in 1:P){
theta.optim = optim(par = log(c(mu[j], inv_disp[j])), fn = gene.LLH,
lambda=lambda, x=X[,j], qgamma=qgamma,
method = "BFGS", hessian = FALSE)
mu[j] = exp(theta.optim$par[1])
inv_disp[j] = exp(theta.optim$par[2])
llh[j] = theta.optim$value
}
mu.update[iter,] = mu
inv_disp.update[iter,] = inv_disp
llh.update[iter] = sum(llh) - sum(log(pi)*t(qgamma))
iter = iter+1
}
plot(llh.update,type = "l")
param = mu.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
lines(c(1:niter),param[,2],col=2)
lines(c(1:niter),param[,3],col=3)
lines(c(1:niter),param[,4],col=4)
param = inv_disp.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
lines(c(1:niter),param[,2],col=2)
lines(c(1:niter),param[,3],col=3)
lines(c(1:niter),param[,4],col=4)
gamma1.est = (qgamma1.update[niter,]>0.5)*1
sum(abs(gamma1.est - gamma.true))
gamma1.est = (qgamma1.update[niter,]>0.8)*1
sum(abs(gamma1.est - gamma.true))
gamma1.est = (qgamma1.update[niter,]>0.5)*1
sum(abs(gamma1.est - gamma.true))
N = 1000
P = 4
K = 2
mu.true = c(5,5,10,10)
inv_disp.true = rep(0.01,P)
pi = c(0.5,0.5)
lambda = c(1,2)
seed = 1
data = simTwoClassNB(N, P, mu.true, inv_disp.true, pi, lambda, seed)
X = data$X
gamma.true = data$gamma1
qgamma.true = Qgamma(X, mu.true, inv_disp.true, pi, lambda)
llh.true = rep(NA,P)
for (j in 1:P){
llh.true[j] = gene.LLH(theta=log(c(mu.true[j],inv_disp.true[j])), lambda, X[,j], qgamma.true)
}
LLH.true = sum(llh.true) - sum(log(pi)*t(qgamma.true))
set.seed(seed)
niter = 20
mu.update = matrix(NA,nrow = niter,ncol = P)
inv_disp.update = matrix(NA,nrow = niter,ncol = P)
qgamma1.update = matrix(NA,nrow = niter,ncol = N)
llh.update = rep(NA,niter)
mu.init = runif(P,min = 0,max = 50)
mu = mu.init
inv_disp.init = runif(P,min = 0,max = 10)
inv_disp = inv_disp.init
iter = 1
while (iter <= niter) {
qgamma = Qgamma(X, mu, inv_disp, pi, lambda)
qgamma1.update[iter,] = qgamma[,1]
llh = rep(NA,P)
for (j in 1:P){
theta.optim = optim(par = log(c(mu[j], inv_disp[j])), fn = gene.LLH,
lambda=lambda, x=X[,j], qgamma=qgamma,
method = "BFGS", hessian = FALSE)
mu[j] = exp(theta.optim$par[1])
inv_disp[j] = exp(theta.optim$par[2])
llh[j] = theta.optim$value
}
mu.update[iter,] = mu
inv_disp.update[iter,] = inv_disp
llh.update[iter] = sum(llh) - sum(log(pi)*t(qgamma))
iter = iter+1
}
plot(llh.update,type = "l")
param = inv_disp.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
lines(c(1:niter),param[,2],col=2)
lines(c(1:niter),param[,3],col=3)
lines(c(1:niter),param[,4],col=4)
## Class 1:
gamma1.est = (qgamma1.update[niter,]>0.5)*1
sum(abs(gamma1.est - gamma.true))
29/1000
(1:N)[qgamma1.update[niter,]<0.5]
(1:N)[abs(gamma1.est - gamma.true)>0]
X[11,]
gamma.true[11]
param = mu_disp.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
lines(c(1:niter),param[,2],col=2)
lines(c(1:niter),param[,3],col=3)
lines(c(1:niter),param[,4],col=4)
param = mu.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
lines(c(1:niter),param[,2],col=2)
lines(c(1:niter),param[,3],col=3)
lines(c(1:niter),param[,4],col=4)
colMeans(X[qgamma1.update[niter,]>0.5])
colMeans(X[qgamma1.update[niter,]>0.5,])
colMeans(X[qgamma1.update[niter,]<0.5,])
set.seed(seed)
niter = 20
mu.update = matrix(NA,nrow = niter,ncol = P)
inv_disp.update = matrix(NA,nrow = niter,ncol = P)
qgamma1.update = matrix(NA,nrow = niter,ncol = N)
llh.update = rep(NA,niter)
mu.init = runif(P,min = 0,max = 50)
mu = mu.init
inv_disp.init = runif(P,min = 0,max = 10)
inv_disp = inv_disp.init
iter = 1
while (iter <= niter) {
qgamma = Qgamma(X, mu, inv_disp, pi, lambda)
qgamma1.update[iter,] = qgamma[,1]
llh = rep(NA,P)
for (j in 1:P){
theta.optim = optim(par = log(c(mu[j], inv_disp[j])), fn = gene.LLH,
lambda=lambda*1.5, x=X[,j], qgamma=qgamma,
method = "BFGS", hessian = FALSE)
mu[j] = exp(theta.optim$par[1])
inv_disp[j] = exp(theta.optim$par[2])
llh[j] = theta.optim$value
}
mu.update[iter,] = mu
inv_disp.update[iter,] = inv_disp
llh.update[iter] = sum(llh) - sum(log(pi)*t(qgamma))
iter = iter+1
}
plot(llh.update,type = "l")
param = mu.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
lines(c(1:niter),param[,2],col=2)
lines(c(1:niter),param[,3],col=3)
lines(c(1:niter),param[,4],col=4)
param = inv_disp.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
lines(c(1:niter),param[,2],col=2)
lines(c(1:niter),param[,3],col=3)
lines(c(1:niter),param[,4],col=4)
gamma1.est = (qgamma1.update[niter,]>0.5)*1
sum(abs(gamma1.est - gamma.true))
set.seed(seed)
niter = 50
mu.update = matrix(NA,nrow = niter,ncol = P)
inv_disp.update = matrix(NA,nrow = niter,ncol = P)
qgamma1.update = matrix(NA,nrow = niter,ncol = N)
llh.update = rep(NA,niter)
mu.init = runif(P,min = 0,max = 50)
mu = mu.init
inv_disp.init = runif(P,min = 0,max = 10)
inv_disp = inv_disp.init
iter = 1
while (iter <= niter) {
qgamma = Qgamma(X, mu, inv_disp, pi, lambda)
qgamma1.update[iter,] = qgamma[,1]
llh = rep(NA,P)
for (j in 1:P){
theta.optim = optim(par = log(c(mu[j], inv_disp[j])), fn = gene.LLH,
lambda=lambda*1.2, x=X[,j], qgamma=qgamma,
method = "BFGS", hessian = FALSE)
mu[j] = exp(theta.optim$par[1])
inv_disp[j] = exp(theta.optim$par[2])
llh[j] = theta.optim$value
}
mu.update[iter,] = mu
inv_disp.update[iter,] = inv_disp
llh.update[iter] = sum(llh) - sum(log(pi)*t(qgamma))
iter = iter+1
}
plot(llh.update,type = "l")
param = mu.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
lines(c(1:niter),param[,2],col=2)
lines(c(1:niter),param[,3],col=3)
lines(c(1:niter),param[,4],col=4)
param = inv_disp.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
lines(c(1:niter),param[,2],col=2)
lines(c(1:niter),param[,3],col=3)
lines(c(1:niter),param[,4],col=4)
gamma1.est = (qgamma1.update[niter,]>0.5)*1
sum(abs(gamma1.est - gamma.true))
plot(dnbinom(seq(0,50,1),mu=5,size=1/0.01))
plot(dnbinom(seq(0,50,1),mu=10,size=1/0.01))
table
table(gamma.true)
table(qgamma1.update[niter,]>0.5)
logNB <- function(x, mu, inv_disp){
#  Returns the sum of the log of NB density over x
#  Args:
#     x : length P vector of gene counts for a cell
#     mu : length P vector of gene means
#     inv_disp : length P vector of gene inverse dispersion parameters
NB <- dnbinom(x, mu=mu, size=1/inv_disp)
return(sum(log(NB)))
}
Qgamma <- function(X, mu, inv_disp, pi, lambda){
#  Returns (N x K) posterior inclusion probabilities of gamma
#  Args:
#     X : matrix of counts for a gene (N x p)
#     mu : vector of gene means (length p)
#     inv_disp : vector of inverse dispersion parameters (length p)
#     pi : mixture proportions (length k)
#     lambda : copy number for this CNV (scalar)
N <- nrow(X)
k <- length(pi)
nb_vals <- rep(0, k)
qik <- matrix(0, nrow=N, ncol=k)
for (i in 1:N){
for(j in 1:k){
nb_vals[j] <- logNB(X[i,], mu=lambda[j]*mu, inv_disp=inv_disp)
}
nb_vals <- nb_vals - min(nb_vals)
qik[i,] <- exp(nb_vals)*pi / sum(exp(nb_vals)*pi)
}
return(qik)
}
gene.LLH <- function(theta, lambda, x, qgamma){
#  Returns the log likelihood of the CNV blocks from N cells
#  Args:
#    theta: length 2 vector that stores: log(mu_j), log(inv_disp_j)
#    lambda: length K vector of fold changes for each class
#    X: length N vector that stores the expression of gene j in each sample
#    pi: length K vector of mixture probabilities for each class
#    qgamma: (N x K) posterior inclusion probabilities of gamma
mu = exp(theta[1])
inv_disp = exp(theta[2])
N = length(x)
K = length(pi)
llh = rep(NA,N)
for (i in 1:N){
llh[i] = sum(qgamma[i,]*(log(dnbinom(x[i], mu=mu*lambda, size=1/inv_disp))))
}
return(-1*sum(llh))
}
simTwoClassNB <- function(N, P, mu, inv_disp, pi, lambda, seed){
#  Returns NxP count matrix sampled from a mixture of NB distributions
#  Args:
#     N: cells (rows)
#     P: genes (columns)
#     mu : vector of gene means (length P)
#     inv_disp : vector of inverse dispersion parameters (length P)
#     pi : mixture proportions (assuming two classes)
#     lambda: copy number change
set.seed(seed)
count.mat <- matrix(0, nrow=N, ncol=P)
gamma = rbinom(N, 1, pi[1])
for (i in 1:N){
count.mat[i,] <- gamma[i] * rnbinom(n = P, mu = lambda[1]*mu, size = 1/inv_disp) +
(1-gamma[i]) * rnbinom(n = P, mu = lambda[2]*mu, size = 1/inv_disp)
}
return(list(X = count.mat, gamma1 = gamma))
}
#####
## Simulate data
N = 1000
P = 10
K = 2
mu.true = c(5,5,5,5,5,10,10,10,10,10)
inv_disp.true = rep(0.01,P)
pi = c(0.5,0.5)
lambda = c(1,2)
seed = 1
data = simTwoClassNB(N, P, mu.true, inv_disp.true, pi, lambda, seed)
X = data$X
gamma.true = data$gamma1
qgamma.true = Qgamma(X, mu.true, inv_disp.true, pi, lambda)
llh.true = rep(NA,P)
for (j in 1:P){
llh.true[j] = gene.LLH(theta=log(c(mu.true[j],inv_disp.true[j])), lambda, X[,j], qgamma.true)
}
LLH.true = sum(llh.true) - sum(log(pi)*t(qgamma.true))
head(qgamma.true)
set.seed(seed)
niter = 50
mu.update = matrix(NA,nrow = niter,ncol = P)
inv_disp.update = matrix(NA,nrow = niter,ncol = P)
qgamma1.update = matrix(NA,nrow = niter,ncol = N)
llh.update = rep(NA,niter)
mu.init = runif(P,min = 0,max = 50)
mu = mu.init
inv_disp.init = runif(P,min = 0,max = 10)
inv_disp = inv_disp.init
iter = 1
while (iter <= niter) {
qgamma = Qgamma(X, mu, inv_disp, pi, lambda)
qgamma1.update[iter,] = qgamma[,1]
llh = rep(NA,P)
for (j in 1:P){
theta.optim = optim(par = log(c(mu[j], inv_disp[j])), fn = gene.LLH,
lambda=lambda*1, x=X[,j], qgamma=qgamma,
method = "BFGS", hessian = FALSE)
mu[j] = exp(theta.optim$par[1])
inv_disp[j] = exp(theta.optim$par[2])
llh[j] = theta.optim$value
}
mu.update[iter,] = mu
inv_disp.update[iter,] = inv_disp
llh.update[iter] = sum(llh) - sum(log(pi)*t(qgamma))
iter = iter+1
}
plot(llh.update,type = "l")
param = mu.update
dim(param)[2]
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
for (v in 2:dim(param)[2]) {
lines(c(1:niter),param[,v],col=v)
}
gamma1.est = (qgamma1.update[niter,]>0.5)*1
sum(abs(gamma1.est - gamma.true))
set.seed(seed)
niter = 50
mu.update = matrix(NA,nrow = niter,ncol = P)
inv_disp.update = matrix(NA,nrow = niter,ncol = P)
qgamma1.update = matrix(NA,nrow = niter,ncol = N)
llh.update = rep(NA,niter)
mu.init = runif(P,min = 0,max = 50)
mu = mu.init
inv_disp.init = runif(P,min = 0,max = 10)
inv_disp = inv_disp.init
iter = 1
while (iter <= niter) {
qgamma = Qgamma(X, mu, inv_disp, pi, lambda)
qgamma1.update[iter,] = qgamma[,1]
llh = rep(NA,P)
for (j in 1:P){
theta.optim = optim(par = log(c(mu[j], inv_disp[j])), fn = gene.LLH,
lambda=lambda*1.5, x=X[,j], qgamma=qgamma,
method = "BFGS", hessian = FALSE)
mu[j] = exp(theta.optim$par[1])
inv_disp[j] = exp(theta.optim$par[2])
llh[j] = theta.optim$value
}
mu.update[iter,] = mu
inv_disp.update[iter,] = inv_disp
llh.update[iter] = sum(llh) - sum(log(pi)*t(qgamma))
iter = iter+1
}
plot(llh.update,type = "l")
param = mu.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
for (v in 2:dim(param)[2]) {
lines(c(1:niter),param[,v],col=v)
}
gamma1.est = (qgamma1.update[niter,]>0.5)*1
sum(abs(gamma1.est - gamma.true))
llh.update[1:5]
mu.update[2,]
inv_disp.update[2,]
qgamma1.update[1,1:10]
qgamma1.update[2,1:10]
table
table( (qgamma1.update[niter,]>0.5))
X[1,]
set.seed(seed)
niter = 50
mu.update = matrix(NA,nrow = niter,ncol = P)
inv_disp.update = matrix(NA,nrow = niter,ncol = P)
qgamma1.update = matrix(NA,nrow = niter,ncol = N)
llh.update = rep(NA,niter)
mu.init = runif(P,min = 0,max = 50)
mu = mu.init
inv_disp.init = runif(P,min = 0,max = 10)
inv_disp = inv_disp.init
iter = 1
while (iter <= niter) {
qgamma = Qgamma(X, mu, inv_disp, pi, lambda)
qgamma1.update[iter,] = qgamma[,1]
llh = rep(NA,P)
for (j in 1:P){
theta.optim = optim(par = log(c(mu[j], inv_disp[j])), fn = gene.LLH,
lambda=lambda*1.1, x=X[,j], qgamma=qgamma,
method = "BFGS", hessian = FALSE)
mu[j] = exp(theta.optim$par[1])
inv_disp[j] = exp(theta.optim$par[2])
llh[j] = theta.optim$value
}
mu.update[iter,] = mu
inv_disp.update[iter,] = inv_disp
llh.update[iter] = sum(llh) - sum(log(pi)*t(qgamma))
iter = iter+1
}
plot(llh.update,type = "l")
param = mu.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
for (v in 2:dim(param)[2]) {
lines(c(1:niter),param[,v],col=v)
}
gamma1.est = (qgamma1.update[niter,]>0.5)*1
sum(abs(gamma1.est - gamma.true))
set.seed(seed)
niter = 50
mu.update = matrix(NA,nrow = niter,ncol = P)
inv_disp.update = matrix(NA,nrow = niter,ncol = P)
qgamma1.update = matrix(NA,nrow = niter,ncol = N)
llh.update = rep(NA,niter)
mu.init = runif(P,min = 0,max = 50)
mu = mu.init
inv_disp.init = runif(P,min = 0,max = 10)
inv_disp = inv_disp.init
iter = 1
while (iter <= niter) {
qgamma = Qgamma(X, mu, inv_disp, pi, lambda)
qgamma1.update[iter,] = qgamma[,1]
llh = rep(NA,P)
for (j in 1:P){
theta.optim = optim(par = log(c(mu[j], inv_disp[j])), fn = gene.LLH,
lambda=lambda*1.2, x=X[,j], qgamma=qgamma,
method = "BFGS", hessian = FALSE)
mu[j] = exp(theta.optim$par[1])
inv_disp[j] = exp(theta.optim$par[2])
llh[j] = theta.optim$value
}
mu.update[iter,] = mu
inv_disp.update[iter,] = inv_disp
llh.update[iter] = sum(llh) - sum(log(pi)*t(qgamma))
iter = iter+1
}
plot(llh.update,type = "l")
param = mu.update
plot(param[,1],type = "l",ylim = c(min(param),max(param)))
for (v in 2:dim(param)[2]) {
lines(c(1:niter),param[,v],col=v)
}
## Class 1:
gamma1.est = (qgamma1.update[niter,]>0.5)*1
sum(abs(gamma1.est - gamma.true))
load("/Users/yifan/Downloads/brainspan_genes_matrix_csv/brainspan_top10k_TOM-block.1.RData")
setwd("~/Downloads/ASoC/singlecell/github")
?grid.arrange
library(gridExtra)
?grid.arrange
